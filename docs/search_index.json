[
["index.html", "Ejercicios APREST Introducción", " Ejercicios APREST Miguel Ángel Porras 2017-02-20 Introducción Serie de ejercicios sobre KNN creados para la asignatura APREST del máster de DS. "],
["problema-de-clasificacion.html", "Ejercicio 1 Problema de clasificación 1.1 Pasos previos 1.2 Conjunto Test y Train 1.3 KNN ponderado con validación cruzada. 1.4 Clasificiación del conjunto test.", " Ejercicio 1 Problema de clasificación 1.1 Pasos previos Para empezar, importamos nuestro archivo de datos datawork.csv. ## Parsed with column specification: ## cols( ## .default = col_double(), ## clasobj = col_character(), ## x30 = col_integer() ## ) ## See spec(...) for full column specifications. Veamos una breve descripción del conjunto de datos. head(df) ## # A tibble: 6 × 42 ## clasobj varobj x01 x02 x03 x04 x05 x06 x07 x08 x09 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 CC 12.63 0.64 0.68 25.06 33.42 103.18 1.08 80.27 14.83 18.09 ## 2 AA 25.08 0.70 0.72 16.08 10.98 38.31 1.62 57.32 30.72 17.13 ## 3 DD 28.84 0.45 0.71 57.78 48.77 148.21 0.34 174.54 50.72 2.72 ## 4 AA 26.58 0.15 0.56 17.79 10.04 30.33 0.86 57.44 167.47 23.09 ## 5 BB 18.53 0.70 0.54 17.48 25.51 82.42 1.04 57.55 40.13 1.70 ## 6 CC 14.20 0.37 0.75 50.01 32.25 99.49 0.92 153.46 1.90 1.76 ## # ... with 31 more variables: x10 &lt;dbl&gt;, x11 &lt;dbl&gt;, x12 &lt;dbl&gt;, x13 &lt;dbl&gt;, ## # x14 &lt;dbl&gt;, x15 &lt;dbl&gt;, x16 &lt;dbl&gt;, x17 &lt;dbl&gt;, x18 &lt;dbl&gt;, x19 &lt;dbl&gt;, ## # x20 &lt;dbl&gt;, x21 &lt;dbl&gt;, x22 &lt;dbl&gt;, x23 &lt;dbl&gt;, x24 &lt;dbl&gt;, x25 &lt;dbl&gt;, ## # x26 &lt;dbl&gt;, x27 &lt;dbl&gt;, x28 &lt;dbl&gt;, x29 &lt;dbl&gt;, x30 &lt;int&gt;, x31 &lt;dbl&gt;, ## # x32 &lt;dbl&gt;, x33 &lt;dbl&gt;, x34 &lt;dbl&gt;, x35 &lt;dbl&gt;, x36 &lt;dbl&gt;, x37 &lt;dbl&gt;, ## # x38 &lt;dbl&gt;, x39 &lt;dbl&gt;, x40 &lt;dbl&gt; summary(df) ## clasobj varobj x01 x02 ## Length:4000 Min. : 10.49 Min. :0.0700 Min. :0.1300 ## Class :character 1st Qu.: 14.35 1st Qu.:0.4000 1st Qu.:0.5100 ## Mode :character Median : 19.34 Median :0.5000 Median :0.6000 ## Mean : 20.86 Mean :0.4989 Mean :0.5977 ## 3rd Qu.: 25.48 3rd Qu.:0.6000 3rd Qu.:0.7000 ## Max. :713.81 Max. :0.9400 Max. :0.9300 ## x03 x04 x05 x06 ## Min. : 0.94 Min. :-0.64 Min. : 2.96 Min. :0.080 ## 1st Qu.: 15.55 1st Qu.:21.56 1st Qu.: 69.82 1st Qu.:0.660 ## Median : 25.98 Median :29.54 Median : 94.06 Median :0.940 ## Mean : 30.42 Mean :29.48 Mean : 93.91 Mean :1.015 ## 3rd Qu.: 40.17 3rd Qu.:37.34 3rd Qu.:117.63 3rd Qu.:1.270 ## Max. :166.17 Max. :62.67 Max. :194.56 Max. :3.260 ## x07 x08 x09 x10 ## Min. : 3.61 Min. : 0.02 Min. : 0.000 Min. : 0.0000 ## 1st Qu.: 51.86 1st Qu.: 5.58 1st Qu.: 1.147 1st Qu.: 0.1000 ## Median : 83.12 Median : 21.86 Median : 2.020 Median : 0.2500 ## Mean : 96.34 Mean : 162.95 Mean : 4.502 Mean : 0.4398 ## 3rd Qu.:125.90 3rd Qu.: 80.82 3rd Qu.: 3.480 3rd Qu.: 0.5700 ## Max. :502.70 Max. :68676.15 Max. :1097.640 Max. :24.1800 ## x11 x12 x13 x14 ## Min. : 0.490 Min. :10.57 Min. : 9.26 Min. :10.21 ## 1st Qu.: 2.158 1st Qu.:15.45 1st Qu.:15.45 1st Qu.:15.40 ## Median : 2.960 Median :17.66 Median :17.60 Median :17.65 ## Mean : 4.197 Mean :17.56 Mean :17.58 Mean :17.57 ## 3rd Qu.: 4.152 3rd Qu.:19.70 3rd Qu.:19.70 3rd Qu.:19.69 ## Max. :688.530 Max. :25.16 Max. :24.86 Max. :24.64 ## x15 x16 x17 x18 ## Min. :10.15 Min. : 9.65 Min. :10.13 Min. :10.47 ## 1st Qu.:15.42 1st Qu.:15.37 1st Qu.:15.46 1st Qu.:15.43 ## Median :17.58 Median :17.55 Median :17.60 Median :17.62 ## Mean :17.55 Mean :17.55 Mean :17.57 Mean :17.57 ## 3rd Qu.:19.68 3rd Qu.:19.66 3rd Qu.:19.67 3rd Qu.:19.67 ## Max. :24.73 Max. :25.50 Max. :25.15 Max. :25.22 ## x19 x20 x21 x22 ## Min. :10.06 Min. :10.26 Min. : 9.59 Min. : 9.44 ## 1st Qu.:15.44 1st Qu.:15.37 1st Qu.:15.45 1st Qu.:15.39 ## Median :17.61 Median :17.66 Median :17.58 Median :17.63 ## Mean :17.58 Mean :17.58 Mean :17.54 Mean :17.57 ## 3rd Qu.:19.72 3rd Qu.:19.67 3rd Qu.:19.63 3rd Qu.:19.71 ## Max. :25.08 Max. :24.95 Max. :25.09 Max. :24.99 ## x23 x24 x25 x26 ## Min. : 9.99 Min. : 9.63 Min. : 9.03 Min. : 9.89 ## 1st Qu.:15.47 1st Qu.:15.41 1st Qu.:15.44 1st Qu.:15.39 ## Median :17.59 Median :17.50 Median :17.64 Median :17.59 ## Mean :17.57 Mean :17.55 Mean :17.55 Mean :17.59 ## 3rd Qu.:19.66 3rd Qu.:19.70 3rd Qu.:19.75 3rd Qu.:19.73 ## Max. :25.29 Max. :25.15 Max. :24.95 Max. :25.05 ## x27 x28 x29 x30 ## Min. : 5.91 Min. : 7.81 Min. :0.020 Min. :10.00 ## 1st Qu.:15.23 1st Qu.:15.92 1st Qu.:0.250 1st Qu.:14.00 ## Median :17.74 Median :18.41 Median :0.360 Median :16.00 ## Mean :17.78 Mean :18.47 Mean :0.375 Mean :16.76 ## 3rd Qu.:20.36 3rd Qu.:21.14 3rd Qu.:0.490 3rd Qu.:19.00 ## Max. :47.94 Max. :28.27 Max. :0.920 Max. :29.00 ## x31 x32 x33 x34 ## Min. : 75.25 Min. : 79.55 Min. : 81.6 Min. : 83.37 ## 1st Qu.:119.00 1st Qu.:118.01 1st Qu.:118.2 1st Qu.:132.24 ## Median :135.91 Median :134.82 Median :135.5 Median :151.30 ## Mean :135.30 Mean :133.66 Mean :134.2 Mean :150.74 ## 3rd Qu.:151.52 3rd Qu.:149.53 3rd Qu.:150.0 3rd Qu.:169.27 ## Max. :191.41 Max. :185.77 Max. :189.7 Max. :241.19 ## x35 x36 x37 x38 ## Min. : 75.3 Min. : 35.80 Min. :-26.19 Min. :203.3 ## 1st Qu.:119.4 1st Qu.: 63.55 1st Qu.: 10.10 1st Qu.:326.0 ## Median :136.8 Median : 73.68 Median : 19.36 Median :374.3 ## Mean :136.5 Mean : 73.64 Mean : 19.57 Mean :372.6 ## 3rd Qu.:154.4 3rd Qu.: 83.84 3rd Qu.: 28.77 3rd Qu.:418.2 ## Max. :197.3 Max. :110.56 Max. :206.12 Max. :534.5 ## x39 x40 ## Min. :210.2 Min. :226.3 ## 1st Qu.:312.5 1st Qu.:349.1 ## Median :357.7 Median :398.4 ## Mean :355.8 Mean :397.5 ## 3rd Qu.:397.9 3rd Qu.:446.0 ## Max. :493.2 Max. :553.8 Observamos que la primera columna la interpreta como un string. Transformemos esta columna a factores. df$clasobj &lt;- as.factor(df$clasobj) Comprobamos que ha leido bien los datos. summary(df) ## clasobj varobj x01 x02 ## AA: 626 Min. : 10.49 Min. :0.0700 Min. :0.1300 ## BB:1315 1st Qu.: 14.35 1st Qu.:0.4000 1st Qu.:0.5100 ## CC:1372 Median : 19.34 Median :0.5000 Median :0.6000 ## DD: 687 Mean : 20.86 Mean :0.4989 Mean :0.5977 ## 3rd Qu.: 25.48 3rd Qu.:0.6000 3rd Qu.:0.7000 ## Max. :713.81 Max. :0.9400 Max. :0.9300 ## x03 x04 x05 x06 ## Min. : 0.94 Min. :-0.64 Min. : 2.96 Min. :0.080 ## 1st Qu.: 15.55 1st Qu.:21.56 1st Qu.: 69.82 1st Qu.:0.660 ## Median : 25.98 Median :29.54 Median : 94.06 Median :0.940 ## Mean : 30.42 Mean :29.48 Mean : 93.91 Mean :1.015 ## 3rd Qu.: 40.17 3rd Qu.:37.34 3rd Qu.:117.63 3rd Qu.:1.270 ## Max. :166.17 Max. :62.67 Max. :194.56 Max. :3.260 ## x07 x08 x09 x10 ## Min. : 3.61 Min. : 0.02 Min. : 0.000 Min. : 0.0000 ## 1st Qu.: 51.86 1st Qu.: 5.58 1st Qu.: 1.147 1st Qu.: 0.1000 ## Median : 83.12 Median : 21.86 Median : 2.020 Median : 0.2500 ## Mean : 96.34 Mean : 162.95 Mean : 4.502 Mean : 0.4398 ## 3rd Qu.:125.90 3rd Qu.: 80.82 3rd Qu.: 3.480 3rd Qu.: 0.5700 ## Max. :502.70 Max. :68676.15 Max. :1097.640 Max. :24.1800 ## x11 x12 x13 x14 ## Min. : 0.490 Min. :10.57 Min. : 9.26 Min. :10.21 ## 1st Qu.: 2.158 1st Qu.:15.45 1st Qu.:15.45 1st Qu.:15.40 ## Median : 2.960 Median :17.66 Median :17.60 Median :17.65 ## Mean : 4.197 Mean :17.56 Mean :17.58 Mean :17.57 ## 3rd Qu.: 4.152 3rd Qu.:19.70 3rd Qu.:19.70 3rd Qu.:19.69 ## Max. :688.530 Max. :25.16 Max. :24.86 Max. :24.64 ## x15 x16 x17 x18 ## Min. :10.15 Min. : 9.65 Min. :10.13 Min. :10.47 ## 1st Qu.:15.42 1st Qu.:15.37 1st Qu.:15.46 1st Qu.:15.43 ## Median :17.58 Median :17.55 Median :17.60 Median :17.62 ## Mean :17.55 Mean :17.55 Mean :17.57 Mean :17.57 ## 3rd Qu.:19.68 3rd Qu.:19.66 3rd Qu.:19.67 3rd Qu.:19.67 ## Max. :24.73 Max. :25.50 Max. :25.15 Max. :25.22 ## x19 x20 x21 x22 ## Min. :10.06 Min. :10.26 Min. : 9.59 Min. : 9.44 ## 1st Qu.:15.44 1st Qu.:15.37 1st Qu.:15.45 1st Qu.:15.39 ## Median :17.61 Median :17.66 Median :17.58 Median :17.63 ## Mean :17.58 Mean :17.58 Mean :17.54 Mean :17.57 ## 3rd Qu.:19.72 3rd Qu.:19.67 3rd Qu.:19.63 3rd Qu.:19.71 ## Max. :25.08 Max. :24.95 Max. :25.09 Max. :24.99 ## x23 x24 x25 x26 ## Min. : 9.99 Min. : 9.63 Min. : 9.03 Min. : 9.89 ## 1st Qu.:15.47 1st Qu.:15.41 1st Qu.:15.44 1st Qu.:15.39 ## Median :17.59 Median :17.50 Median :17.64 Median :17.59 ## Mean :17.57 Mean :17.55 Mean :17.55 Mean :17.59 ## 3rd Qu.:19.66 3rd Qu.:19.70 3rd Qu.:19.75 3rd Qu.:19.73 ## Max. :25.29 Max. :25.15 Max. :24.95 Max. :25.05 ## x27 x28 x29 x30 ## Min. : 5.91 Min. : 7.81 Min. :0.020 Min. :10.00 ## 1st Qu.:15.23 1st Qu.:15.92 1st Qu.:0.250 1st Qu.:14.00 ## Median :17.74 Median :18.41 Median :0.360 Median :16.00 ## Mean :17.78 Mean :18.47 Mean :0.375 Mean :16.76 ## 3rd Qu.:20.36 3rd Qu.:21.14 3rd Qu.:0.490 3rd Qu.:19.00 ## Max. :47.94 Max. :28.27 Max. :0.920 Max. :29.00 ## x31 x32 x33 x34 ## Min. : 75.25 Min. : 79.55 Min. : 81.6 Min. : 83.37 ## 1st Qu.:119.00 1st Qu.:118.01 1st Qu.:118.2 1st Qu.:132.24 ## Median :135.91 Median :134.82 Median :135.5 Median :151.30 ## Mean :135.30 Mean :133.66 Mean :134.2 Mean :150.74 ## 3rd Qu.:151.52 3rd Qu.:149.53 3rd Qu.:150.0 3rd Qu.:169.27 ## Max. :191.41 Max. :185.77 Max. :189.7 Max. :241.19 ## x35 x36 x37 x38 ## Min. : 75.3 Min. : 35.80 Min. :-26.19 Min. :203.3 ## 1st Qu.:119.4 1st Qu.: 63.55 1st Qu.: 10.10 1st Qu.:326.0 ## Median :136.8 Median : 73.68 Median : 19.36 Median :374.3 ## Mean :136.5 Mean : 73.64 Mean : 19.57 Mean :372.6 ## 3rd Qu.:154.4 3rd Qu.: 83.84 3rd Qu.: 28.77 3rd Qu.:418.2 ## Max. :197.3 Max. :110.56 Max. :206.12 Max. :534.5 ## x39 x40 ## Min. :210.2 Min. :226.3 ## 1st Qu.:312.5 1st Qu.:349.1 ## Median :357.7 Median :398.4 ## Mean :355.8 Mean :397.5 ## 3rd Qu.:397.9 3rd Qu.:446.0 ## Max. :493.2 Max. :553.8 Por último veamos cuantos datos tenemos. dim(df) ## [1] 4000 42 1.2 Conjunto Test y Train Fijamos una semilla para trabajar. set.seed(2017) Seleccionamos los datos, los normalizamos, y los guardamos en las variables df.train y df.test. library(rknn) ## Loading required package: gmp ## ## Attaching package: &#39;gmp&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## %*%, apply, crossprod, matrix, tcrossprod n &lt;- dim(df)[1] val &lt;- sample(1:n, size = round(n/3), replace = FALSE, prob = rep(1/n, n)) df.train &lt;- data.frame(clasobj=df$clasobj[-val], varobj=df$varobj[-val], normalize.unit(df[-val,c(-1,-2)])) df.test &lt;- data.frame(clasobj=df$clasobj[val], varobj=df$varobj[val], normalize.unit(df[val,c(-1,-2)])) 1.3 KNN ponderado con validación cruzada. Usamos la función train.kknn de la librería kknn. library(kknn) (fit.train1 &lt;- train.kknn(clasobj ~ ., df.train[,-2], kmax = 20, kernel = c(&quot;triangular&quot;, &quot;rectangular&quot;, &quot;epanechnikov&quot;, &quot;optimal&quot;, &quot;gaussian&quot;, &quot;rank&quot;,&quot;inv&quot;), distance = 2)) ## ## Call: ## train.kknn(formula = clasobj ~ ., data = df.train[, -2], kmax = 20, distance = 2, kernel = c(&quot;triangular&quot;, &quot;rectangular&quot;, &quot;epanechnikov&quot;, &quot;optimal&quot;, &quot;gaussian&quot;, &quot;rank&quot;, &quot;inv&quot;)) ## ## Type of response variable: nominal ## Minimal misclassification: 0.01574803 ## Best kernel: triangular ## Best k: 13 Nos dice que el mejor kernel es el rectangular con k igual a 4. fit.train1$best.parameters ## $kernel ## [1] &quot;triangular&quot; ## ## $k ## [1] 13 fit.train1$response ## [1] &quot;nominal&quot; Veamos una representación de los kernels respecto los k. plot(fit.train1) 1.4 Clasificiación del conjunto test. Las predicciones para df.test son las siguientes. (df.pred1 &lt;-predict(fit.train1, df.test[,-2])) ## [1] CC CC AA CC BB DD BB BB CC AA DD CC AA AA CC CC BB CC CC DD BB BB CC ## [24] AA CC DD BB CC DD CC CC BB AA CC CC CC BB CC DD BB DD CC BB CC CC BB ## [47] CC AA BB AA BB BB BB CC CC DD BB AA CC DD BB CC CC CC CC BB CC AA CC ## [70] CC BB DD BB CC DD CC BB CC CC CC BB AA BB CC CC CC BB CC BB BB BB DD ## [93] AA DD AA CC CC AA DD DD CC BB AA BB CC BB DD AA DD CC BB CC DD CC CC ## [116] AA DD CC BB BB AA CC BB DD CC AA CC AA CC BB CC BB DD BB DD BB BB CC ## [139] DD BB DD BB BB AA DD DD BB BB BB DD CC CC CC BB BB CC BB CC CC AA DD ## [162] CC DD BB CC BB BB CC BB CC BB AA BB DD BB DD CC CC BB BB AA AA BB CC ## [185] BB DD BB BB CC BB AA AA BB AA BB DD BB BB BB CC CC BB BB CC CC BB BB ## [208] CC CC BB BB BB CC DD DD CC CC BB CC CC CC BB AA BB CC BB BB DD CC DD ## [231] BB CC BB CC CC BB BB BB BB CC AA CC BB BB BB CC BB CC BB BB DD BB CC ## [254] BB CC CC CC AA BB CC CC BB CC BB BB DD BB DD CC AA BB BB BB AA CC CC ## [277] AA CC BB CC DD BB DD CC BB DD CC AA BB DD CC CC BB CC CC DD CC DD BB ## [300] DD BB BB CC AA AA BB BB DD BB BB CC CC BB BB BB CC CC AA BB BB CC BB ## [323] CC CC DD CC BB AA BB DD AA CC AA DD BB BB AA BB CC BB CC CC CC AA BB ## [346] DD BB CC AA CC CC AA CC AA CC AA DD CC BB AA DD DD CC DD DD BB CC BB ## [369] AA BB CC AA CC AA AA AA CC CC CC CC BB DD CC BB CC AA CC AA CC CC BB ## [392] CC BB BB CC DD CC BB CC BB DD CC DD AA BB CC CC CC DD BB BB CC CC DD ## [415] AA AA BB BB DD AA BB CC CC CC CC AA DD CC CC BB DD BB BB BB BB BB AA ## [438] BB DD BB CC BB BB BB BB BB BB CC CC CC BB BB AA AA DD BB DD BB BB CC ## [461] CC CC BB CC BB BB BB CC CC CC AA AA AA BB DD CC BB AA CC BB BB CC BB ## [484] AA DD CC BB AA CC BB AA DD AA DD AA BB CC BB DD AA CC AA BB BB AA CC ## [507] AA CC AA BB BB DD AA CC CC BB BB CC CC BB DD DD AA CC DD BB CC CC CC ## [530] CC BB AA CC AA BB BB CC CC BB BB BB BB AA AA CC BB AA AA BB DD AA AA ## [553] BB DD AA DD AA BB AA BB BB CC CC CC CC BB AA DD AA AA BB BB BB BB CC ## [576] AA BB CC BB BB CC BB CC DD DD CC AA BB BB BB AA BB BB BB CC DD BB DD ## [599] CC CC AA CC BB BB BB CC DD BB BB BB AA BB DD CC AA DD DD DD BB AA CC ## [622] BB DD BB BB BB DD DD BB CC BB BB BB CC CC AA BB DD BB CC CC CC BB BB ## [645] DD DD AA CC BB CC AA CC BB CC BB CC AA DD CC BB DD BB CC BB BB CC BB ## [668] BB CC CC BB BB DD CC AA CC BB CC BB CC AA CC CC BB BB CC AA BB BB BB ## [691] BB CC BB BB AA CC AA CC CC DD CC DD CC DD CC AA DD CC CC DD CC CC CC ## [714] BB CC CC BB BB BB CC CC CC CC CC AA BB AA CC BB BB BB DD BB CC AA BB ## [737] CC BB CC CC DD CC BB CC BB BB CC AA AA DD DD AA BB AA BB BB CC AA CC ## [760] DD BB CC DD BB CC CC DD CC AA DD CC BB AA AA CC DD BB DD BB CC CC DD ## [783] BB BB BB CC BB CC CC BB AA BB BB CC BB CC CC BB BB CC BB BB CC DD DD ## [806] AA CC DD CC CC CC BB AA BB BB CC CC BB CC BB AA BB BB CC BB DD CC BB ## [829] CC DD DD DD CC AA BB CC BB CC CC CC BB DD DD BB CC CC CC CC BB DD BB ## [852] BB CC DD BB AA DD DD AA BB DD CC BB BB AA DD DD CC BB AA BB AA BB CC ## [875] CC BB DD DD DD DD DD AA CC BB CC AA DD BB BB AA AA BB CC CC DD CC BB ## [898] CC CC CC AA CC BB CC CC CC CC DD BB DD BB BB DD BB CC DD CC CC CC BB ## [921] AA BB BB BB DD AA CC CC AA DD AA AA CC CC BB CC CC BB DD CC BB AA AA ## [944] DD BB CC BB DD BB BB BB BB CC BB AA BB BB BB AA CC CC BB CC AA CC BB ## [967] BB BB BB CC CC CC CC CC CC AA BB AA DD CC CC CC BB CC AA DD AA AA BB ## [990] BB BB BB AA BB CC BB BB CC CC DD CC BB AA BB BB BB DD CC BB AA BB BB ## [1013] BB CC DD CC BB BB DD CC BB BB BB BB BB DD CC CC DD DD DD CC CC BB AA ## [1036] CC BB AA DD DD CC BB AA BB CC BB CC AA CC AA CC AA CC DD CC CC DD CC ## [1059] CC BB CC DD CC CC BB BB AA AA BB BB CC AA CC DD DD CC AA AA BB AA BB ## [1082] CC BB BB BB CC BB DD BB AA BB AA DD DD DD DD AA AA CC BB DD BB BB DD ## [1105] DD CC AA DD CC BB BB DD BB BB CC CC BB AA BB CC AA CC BB DD DD AA CC ## [1128] AA DD BB AA BB DD BB CC CC CC BB CC DD BB BB BB CC BB AA CC CC BB DD ## [1151] DD AA AA AA AA CC BB CC AA CC DD BB DD BB CC CC CC CC AA AA AA BB AA ## [1174] AA CC DD CC DD BB DD CC BB CC CC DD CC DD BB DD BB CC DD BB CC AA BB ## [1197] CC CC BB BB AA CC CC BB CC DD CC BB CC CC CC DD CC DD AA AA CC BB CC ## [1220] BB DD BB BB AA BB CC CC AA DD CC BB BB CC AA CC DD DD BB BB CC CC BB ## [1243] DD AA BB DD DD BB BB DD CC DD DD BB BB BB BB BB DD BB CC DD DD DD BB ## [1266] DD CC CC DD CC BB DD BB CC AA BB DD DD BB AA DD BB BB DD AA BB DD CC ## [1289] BB CC AA DD CC BB CC BB CC CC AA AA BB BB DD CC AA BB CC BB DD BB BB ## [1312] BB BB CC BB AA BB BB CC CC CC BB DD CC AA DD AA DD AA CC BB BB DD ## Levels: AA BB CC DD Veamos como de bueno es el modelo con la matriz de confusión. (table.conf &lt;- table(df.pred1, df.test$clasobj)) ## ## df.pred1 AA BB CC DD ## AA 201 1 0 4 ## BB 1 452 9 2 ## CC 0 3 435 2 ## DD 1 1 1 220 La tasa de error para el conjunto test es, (table.conf[1,2] + table.conf[1,3] + table.conf[1,4] + table.conf[2,1] + table.conf[2,3] + table.conf[2,4] + table.conf[3,1] + table.conf[3,2] + table.conf[3,4] + table.conf[4,1] + table.conf[4,2] + table.conf[4,3]) / dim(df.test)[1] ## [1] 0.01875469 Observamos que la proporción de error es muy pequeña. El error de clasificación para el caso óptimo es, fit.train1$MISCLASS[fit.train1$best.parameters$k,fit.train1$best.parameters$kernel] ## [1] 0.01574803 "],
["problema-de-regresion.html", "Ejercicio 2 Problema de regresión 2.1 Conjunto Test y Train 2.2 KNN ponderado con validación cruzada. 2.3 Clasificiación del conjunto test. 2.4 KNN aleatorio", " Ejercicio 2 Problema de regresión 2.1 Conjunto Test y Train Fijamos una semilla para trabajar. set.seed(2017) Seleccionamos los datos, los normalizamos, y los guardamos en las variables df.train y df.test. library(readr) library(rknn) ## Loading required package: gmp ## ## Attaching package: &#39;gmp&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## %*%, apply, crossprod, matrix, tcrossprod df &lt;- read_delim(&quot;datawork.csv&quot;, &quot;;&quot;, escape_double = FALSE, trim_ws = TRUE) ## Parsed with column specification: ## cols( ## .default = col_double(), ## clasobj = col_character(), ## x30 = col_integer() ## ) ## See spec(...) for full column specifications. n &lt;- dim(df)[1] val &lt;- sample(1:n, size = round(n/3), replace = FALSE, prob = rep(1/n, n)) df.train &lt;- data.frame(clasobj=df$clasobj[-val], varobj=df$varobj[-val], normalize.unit(df[-val,c(-1,-2)])) df.test &lt;- data.frame(clasobj=df$clasobj[val], varobj=df$varobj[val], normalize.unit(df[val,c(-1,-2)])) 2.2 KNN ponderado con validación cruzada. Usamos la función train.kknn de la librería kknn. library(kknn) set.seed(2017) (fit.train2 &lt;- train.kknn(varobj ~ ., df.train[,-1], kmax = 20, kernel = c(&quot;triangular&quot;, &quot;rectangular&quot;, &quot;epanechnikov&quot;, &quot;optimal&quot;, &quot;gaussian&quot;, &quot;rank&quot;,&quot;inv&quot;), distance = 2)) ## ## Call: ## train.kknn(formula = varobj ~ ., data = df.train[, -1], kmax = 20, distance = 2, kernel = c(&quot;triangular&quot;, &quot;rectangular&quot;, &quot;epanechnikov&quot;, &quot;optimal&quot;, &quot;gaussian&quot;, &quot;rank&quot;, &quot;inv&quot;)) ## ## Type of response variable: continuous ## minimal mean absolute error: 1.324149 ## Minimal mean squared error: 7.355905 ## Best kernel: triangular ## Best k: 4 Nos dice que el mejor kernel es el optimal con k igual a 4. fit.train2$best.parameters ## $kernel ## [1] &quot;triangular&quot; ## ## $k ## [1] 4 fit.train2$response ## [1] &quot;continuous&quot; Veamos una representación de los kernels respecto los k. plot(fit.train2) 2.3 Clasificiación del conjunto test. Las predicciones para df.test son las siguientes. (df.pred2 &lt;-predict(fit.train2, df.test[,-2])) ## [1] 12.95868 13.95075 24.44228 13.55820 19.99395 26.42297 ## [7] 19.84820 23.99533 12.70577 25.10359 27.73197 12.47743 ## [13] 24.55840 25.09579 12.54290 12.17659 19.99960 13.94365 ## [19] 13.75631 28.62102 19.15621 19.13291 11.62818 24.47522 ## [25] 12.28137 27.77359 19.98536 13.28443 26.29270 12.73455 ## [31] 12.27219 19.60822 26.19051 13.42992 15.50310 14.51701 ## [37] 18.81795 13.36644 27.40199 19.86800 26.93949 13.43375 ## [43] 18.34338 12.35263 12.64626 18.80810 13.49344 24.69147 ## [49] 18.35292 25.01257 19.92874 19.17263 18.85672 14.10685 ## [55] 12.22182 28.02863 19.34148 20.86344 12.11282 29.57682 ## [61] 18.89303 13.60545 12.45152 13.44475 14.34130 18.47374 ## [67] 12.12728 25.17080 13.65448 13.19704 21.08037 26.40875 ## [73] 20.19778 16.11026 29.00693 12.95106 18.83709 12.74949 ## [79] 12.45948 14.68899 19.15517 25.31155 19.16298 15.53237 ## [85] 14.13802 24.38791 23.07306 12.30551 19.35392 17.68801 ## [91] 19.18819 27.42536 24.74343 27.83953 26.06755 13.98634 ## [97] 18.18097 25.28830 27.83942 27.89986 13.51581 19.68976 ## [103] 25.18332 19.37829 14.62106 20.30802 26.44407 25.87553 ## [109] 29.91763 13.70422 19.43146 12.80927 27.04201 12.39866 ## [115] 12.68281 21.40960 27.14651 13.84733 188.25129 19.47923 ## [121] 20.87235 13.20737 19.50777 28.14991 14.44887 24.49607 ## [127] 12.16113 21.12159 14.15930 19.73858 12.54019 20.50540 ## [133] 29.49471 19.33597 28.01125 18.55958 18.68969 14.47748 ## [139] 26.21339 19.60090 26.63275 19.28188 18.33537 24.91897 ## [145] 29.55134 28.49089 22.45711 20.23163 19.38815 26.03660 ## [151] 12.57827 14.15313 12.76657 19.48541 18.76090 12.93874 ## [157] 19.13993 14.36547 12.68433 24.65269 25.81108 12.48307 ## [163] 28.26235 19.24405 11.94631 19.93123 19.90480 12.25224 ## [169] 19.25864 12.85275 20.35023 24.55017 18.77839 27.82862 ## [175] 19.24153 26.70822 12.28742 14.78651 19.90164 21.50748 ## [181] 25.45578 24.79785 18.86534 14.63136 18.37273 28.35157 ## [187] 19.44684 18.80147 15.61852 19.35581 27.18926 25.27517 ## [193] 19.12761 24.94407 19.00787 26.48095 19.68071 18.99075 ## [199] 18.78425 13.35833 13.48777 18.20626 17.93817 12.54259 ## [205] 13.79713 20.49274 20.22467 13.44733 13.65696 20.22794 ## [211] 19.40378 18.75336 12.29155 27.47859 28.28125 15.14233 ## [217] 13.45470 21.74069 21.73124 14.60841 13.36782 21.36177 ## [223] 25.39234 19.37573 12.00379 19.58628 18.89731 30.14034 ## [229] 12.38135 26.84576 19.68072 13.65687 19.28266 13.44194 ## [235] 13.75618 19.52557 19.32337 19.74600 19.85914 11.77911 ## [241] 24.98404 12.81380 18.52173 19.85762 17.66724 13.50312 ## [247] 19.80283 13.31821 18.33998 19.45077 27.37257 19.63090 ## [253] 13.68436 21.30440 15.45801 13.64666 13.20154 26.02067 ## [259] 19.99806 12.32303 13.62333 19.22455 13.56571 20.08373 ## [265] 19.14927 28.66161 19.08957 27.82399 14.21464 25.06105 ## [271] 18.39660 20.66995 18.71491 24.55866 14.11891 13.77256 ## [277] 20.87244 15.91870 18.83370 11.90733 26.84971 20.89672 ## [283] 28.36605 13.62238 18.88136 26.33290 12.24945 21.14056 ## [289] 18.41442 26.46668 13.18005 13.72027 20.22695 13.05951 ## [295] 14.62541 29.24004 12.84865 28.50807 19.11107 27.84530 ## [301] 19.34712 19.28938 15.82449 26.16882 24.29688 18.68133 ## [307] 20.26048 27.33015 18.90064 19.07131 13.96440 12.74758 ## [313] 18.89055 19.48143 18.73773 13.75419 14.90125 25.09205 ## [319] 18.98484 20.48339 15.09432 18.41282 14.24288 12.10842 ## [325] 28.87353 14.56200 18.61210 25.06655 18.65232 27.39461 ## [331] 24.41901 12.31137 24.54633 27.02028 19.62270 19.23419 ## [337] 25.58000 19.57063 13.42529 18.68127 13.70106 12.58730 ## [343] 12.91090 25.26579 17.76248 29.36076 18.38156 15.66279 ## [349] 24.44366 12.60889 13.66955 25.09345 17.80574 24.26216 ## [355] 13.17870 24.95939 26.32406 13.39862 19.71410 24.69473 ## [361] 25.99249 27.93588 13.60761 28.11058 26.11902 18.72344 ## [367] 13.17895 20.14625 20.35184 19.20006 14.90963 27.60137 ## [373] 14.67804 26.19475 24.68536 24.00568 14.50633 12.84645 ## [379] 12.15166 13.78505 19.15348 28.40962 11.89592 18.81271 ## [385] 12.66463 24.99542 14.50867 24.40314 15.39629 12.59200 ## [391] 18.59667 13.91855 19.13104 18.57540 13.81888 31.20190 ## [397] 13.73400 19.43059 14.01003 20.72015 28.12508 13.27863 ## [403] 26.29560 24.63748 18.81379 13.58378 14.43608 14.52599 ## [409] 28.92785 18.03348 18.69518 12.42503 16.83465 27.37223 ## [415] 27.63207 24.96771 20.04903 20.20605 26.89448 25.24033 ## [421] 18.58570 13.62684 13.84895 12.50560 12.01235 24.11872 ## [427] 27.84533 16.28882 11.98276 19.85682 28.81861 19.37395 ## [433] 19.26043 18.54938 18.76161 17.97013 24.62401 20.39926 ## [439] 27.89083 19.41034 14.51035 20.61176 19.59478 19.88999 ## [445] 18.43492 19.27706 18.75383 13.26999 16.69641 13.33763 ## [451] 18.98328 19.05502 24.16121 25.57641 28.54642 20.18345 ## [457] 27.15465 20.12212 18.90748 13.81142 12.62575 13.21932 ## [463] 19.37351 11.78469 18.65273 19.88937 18.73348 15.61413 ## [469] 13.19719 12.90956 24.92265 22.79770 26.00202 19.55156 ## [475] 27.51339 14.01283 19.60592 24.84516 13.65538 20.33254 ## [481] 20.06427 12.97304 19.24067 26.28188 26.03431 13.66150 ## [487] 18.64419 24.69472 11.65021 18.86965 27.10521 26.82936 ## [493] 24.13948 28.85326 22.08785 19.38897 13.33510 18.76994 ## [499] 27.57897 24.48960 14.41934 25.17356 19.07038 20.98361 ## [505] 24.89737 12.90779 22.44572 12.05190 25.02872 18.86518 ## [511] 20.78395 26.07852 24.71546 13.41461 12.77432 17.89496 ## [517] 18.54625 13.21353 12.97034 18.21520 29.63695 28.62158 ## [523] 25.25638 12.53985 26.38132 19.86947 14.23915 12.77329 ## [529] 14.42450 12.43931 19.09276 24.51141 13.15930 22.89303 ## [535] 19.25348 19.27029 13.50055 13.17686 18.46384 19.82231 ## [541] 18.82325 19.08198 24.72867 25.32640 12.86658 18.95179 ## [547] 24.78026 24.80110 18.39548 28.74477 24.02053 25.09035 ## [553] 19.72945 28.66386 26.05413 25.96101 25.19788 19.66965 ## [559] 24.34832 20.28390 18.37351 13.88121 12.69893 13.07583 ## [565] 12.48568 20.24924 22.81733 27.18761 26.33072 24.51599 ## [571] 19.56748 17.66306 18.34028 18.47344 14.19577 24.94508 ## [577] 19.98694 13.76332 18.48833 19.64737 13.12865 18.40581 ## [583] 14.44303 26.96738 29.33692 12.24737 25.22612 18.05707 ## [589] 19.24758 24.66522 22.03206 19.89453 18.98426 19.11983 ## [595] 13.39528 29.48896 17.73000 27.25456 13.30453 12.72086 ## [601] 24.16453 13.03482 19.42262 19.36480 19.19910 12.49422 ## [607] 27.73361 25.60083 19.83186 18.83630 19.61126 19.49165 ## [613] 28.22985 13.15781 24.34673 26.84342 27.95961 27.23365 ## [619] 20.32242 25.59490 13.04587 21.36009 25.84229 19.73984 ## [625] 24.50360 18.69702 27.88229 27.21537 19.51792 13.39682 ## [631] 18.64138 18.68579 18.80090 16.33790 13.77160 24.94611 ## [637] 18.94710 26.56487 18.94784 14.47753 12.93596 14.10525 ## [643] 20.06844 19.33152 26.74629 28.80505 24.70187 12.05374 ## [649] 18.13705 12.09790 21.82489 14.50406 19.29246 12.00649 ## [655] 19.66375 14.22042 25.52160 28.40126 13.51988 18.42022 ## [661] 27.43151 19.87732 12.85358 19.09768 18.99060 11.58634 ## [667] 18.17667 17.63808 15.24301 13.13466 19.41789 19.44926 ## [673] 27.05630 13.83185 19.62082 15.52696 18.48058 12.81189 ## [679] 20.41033 12.48920 24.75022 14.17030 13.44833 20.70559 ## [685] 19.76789 14.15849 24.24127 19.42950 19.98636 19.29341 ## [691] 18.37357 13.06624 18.74194 19.54932 25.05386 12.79938 ## [697] 24.99781 13.30914 13.56557 29.57204 13.03263 27.57370 ## [703] 13.37130 27.59699 13.25927 25.28466 27.82378 12.78483 ## [709] 14.45947 26.97177 14.54146 15.75342 13.21309 19.81744 ## [715] 13.28716 13.41427 18.00241 18.21041 17.86818 15.57407 ## [721] 13.47221 13.99783 13.29176 15.36656 24.35973 19.12727 ## [727] 25.13380 12.92767 18.85555 20.42260 17.75171 27.16605 ## [733] 17.88184 11.93767 27.36922 19.05188 12.48193 19.03800 ## [739] 14.58760 13.19445 27.33007 12.94454 18.84092 12.72167 ## [745] 18.89248 19.56980 12.26555 25.53698 26.03936 27.76591 ## [751] 30.53808 25.22312 18.85226 24.91472 18.38088 18.88106 ## [757] 14.63530 23.65591 14.44767 27.35970 22.95574 12.61526 ## [763] 27.57321 21.45637 13.47521 14.45009 28.12679 13.03374 ## [769] 22.62050 26.01210 13.56525 18.21963 26.85311 24.18166 ## [775] 13.06530 31.15627 23.18161 27.54906 19.21107 12.66508 ## [781] 12.70806 28.83161 17.60817 20.45986 18.83198 15.40326 ## [787] 20.30257 13.60017 13.05965 19.18726 26.19078 19.19408 ## [793] 18.63569 12.39753 19.32122 13.07727 13.59720 18.82804 ## [799] 19.74602 12.63290 19.33324 18.23921 14.76749 28.17941 ## [805] 26.65572 24.56355 14.77852 26.66277 12.43262 12.61274 ## [811] 13.07731 18.44538 23.32861 19.43559 18.96387 14.93903 ## [817] 13.36135 23.75293 12.66263 18.14971 21.02375 19.37996 ## [823] 17.94870 12.62645 18.81337 26.00288 13.82808 20.42491 ## [829] 13.34492 27.77394 29.11936 27.31738 12.45234 22.97785 ## [835] 19.90437 13.44387 18.97956 15.02270 13.16754 13.13148 ## [841] 19.75501 26.23155 27.80051 19.61505 11.97870 14.35186 ## [847] 13.66292 12.66607 19.49803 28.88528 23.59505 18.18736 ## [853] 14.21395 27.65038 18.62762 25.24081 26.05052 29.18092 ## [859] 24.77186 18.93598 26.91140 12.42060 18.28179 18.18915 ## [865] 24.95324 28.46777 26.52586 13.60201 18.86608 22.78951 ## [871] 19.72459 25.19810 18.27619 12.12710 12.01215 19.68041 ## [877] 29.68352 28.95005 28.39634 27.66868 26.19992 24.92385 ## [883] 13.74955 19.44640 11.94633 24.49176 27.06015 19.38043 ## [889] 18.28664 17.01976 24.90453 19.45021 13.57971 13.94427 ## [895] 29.08784 12.36482 19.47231 13.60800 13.34885 13.50336 ## [901] 25.94438 12.83898 20.56089 13.42471 12.87278 13.70490 ## [907] 13.47600 26.10226 18.08770 28.77295 19.14993 19.75503 ## [913] 26.71347 20.01361 13.16385 29.45720 12.47370 16.13161 ## [919] 13.21830 19.72536 25.58185 18.60175 20.03766 19.89344 ## [925] 26.36858 25.11871 12.55645 12.45402 24.49824 26.34242 ## [931] 24.26019 24.39442 12.25923 14.23296 21.15910 13.35620 ## [937] 13.20523 18.61569 26.22453 12.03592 18.11004 25.48339 ## [943] 25.63637 27.20114 18.50617 14.64952 19.46173 27.76237 ## [949] 19.09776 18.73025 20.94112 19.84843 12.97150 19.47479 ## [955] 25.40825 20.63591 19.69200 19.46672 25.14699 13.63360 ## [961] 13.87729 20.04678 13.87729 24.57208 11.87068 18.67981 ## [967] 20.81677 19.84043 19.73464 11.81690 12.65524 16.69717 ## [973] 12.48837 13.53983 15.88893 25.90177 18.19387 24.40036 ## [979] 28.62997 13.52849 12.37246 12.06637 19.59954 12.97127 ## [985] 22.05436 25.84111 24.99485 24.42953 18.86066 20.07978 ## [991] 20.77100 19.02986 24.76361 18.04300 14.47892 19.77538 ## [997] 19.20493 12.68690 12.82234 28.85167 14.13300 19.85455 ## [1003] 24.94480 18.24526 24.67258 18.55038 27.25587 13.74290 ## [1009] 19.42868 24.18232 19.11133 20.31678 19.85725 13.56933 ## [1015] 28.06395 12.64045 19.78996 18.84176 29.59690 13.96346 ## [1021] 21.64703 19.35603 17.92723 19.40595 17.71319 26.25452 ## [1027] 12.26923 15.20716 29.13133 27.83223 27.68526 12.32366 ## [1033] 13.30240 21.57541 24.93123 12.56446 17.96229 24.80380 ## [1039] 26.19603 28.29383 12.33654 18.95853 27.55726 19.51809 ## [1045] 12.72627 19.56788 12.46012 24.73588 12.70006 24.82420 ## [1051] 11.91556 23.87397 12.43195 26.15679 14.20437 12.53208 ## [1057] 26.26998 12.81262 13.87644 20.02940 14.43808 27.38297 ## [1063] 12.32414 12.76907 18.83200 19.20052 24.15995 24.23495 ## [1069] 19.14530 18.70632 13.87652 24.71322 12.21241 28.93334 ## [1075] 27.82898 13.30721 23.37365 25.74887 18.67242 27.60728 ## [1081] 19.37489 13.16426 19.89527 18.90327 20.03618 13.61966 ## [1087] 18.07568 30.15118 18.98697 26.09280 18.81167 24.49470 ## [1093] 26.18304 27.00210 29.45459 26.88723 24.81358 24.48718 ## [1099] 13.40410 19.96521 30.46970 18.70910 19.00536 27.03556 ## [1105] 28.52199 12.88719 27.44922 26.65184 11.98213 18.53134 ## [1111] 19.54306 28.32993 18.16873 19.27476 12.43148 12.73743 ## [1117] 17.77366 26.88615 19.32961 12.43995 25.02899 12.24545 ## [1123] 19.29298 27.28731 28.99582 24.16025 15.16350 26.02823 ## [1129] 25.88385 18.57577 25.15089 20.34702 27.91387 19.20286 ## [1135] 13.78628 13.31487 15.07520 18.73451 12.71744 26.55273 ## [1141] 19.45003 18.86990 20.21959 14.06989 19.39218 24.60471 ## [1147] 15.47359 13.39962 19.73733 26.13599 27.16016 27.56668 ## [1153] 23.09421 23.87857 24.67699 12.88412 18.39999 14.28112 ## [1159] 24.50556 12.93968 27.17044 21.09575 26.76374 19.53005 ## [1165] 12.18153 14.87549 13.73282 12.77984 24.93930 24.22228 ## [1171] 24.75855 19.79336 25.40181 24.38111 14.44744 27.19688 ## [1177] 12.12542 26.93373 17.98742 29.34160 12.98710 20.67815 ## [1183] 13.18410 12.74623 26.28503 13.41908 28.79212 18.86915 ## [1189] 27.04946 20.64460 13.50902 29.32863 19.22837 15.13651 ## [1195] 21.13041 18.44973 14.25398 12.52425 19.24817 18.11560 ## [1201] 24.64613 12.54246 12.48741 21.42363 14.85204 29.04388 ## [1207] 14.15624 19.20916 12.21108 12.62115 12.02026 29.17642 ## [1213] 12.37204 27.67113 24.84509 25.10453 13.70010 18.97566 ## [1219] 12.55745 19.15159 28.22786 18.48273 20.04034 25.94062 ## [1225] 18.73117 12.07726 13.48232 20.82653 27.20068 16.12702 ## [1231] 19.37607 18.77154 12.64200 23.91227 12.65541 27.53667 ## [1237] 29.94262 21.27878 18.86552 13.69551 13.76244 18.97570 ## [1243] 27.68051 25.69273 18.42594 28.88497 26.88542 18.66678 ## [1249] 24.83507 27.91634 12.41522 26.83356 28.85981 18.77870 ## [1255] 19.66188 19.44291 19.18297 19.38668 28.62267 19.29283 ## [1261] 12.95106 26.72477 29.45346 26.25689 19.22594 27.65027 ## [1267] 13.12484 14.06423 27.76297 12.16569 18.78861 28.84031 ## [1273] 19.41673 12.69410 23.72936 19.41816 28.91673 27.78152 ## [1279] 18.61593 24.60343 26.04138 19.54304 18.73061 29.31566 ## [1285] 24.03352 18.75692 28.45191 15.20727 20.09217 13.30847 ## [1291] 26.19000 26.47320 12.45407 19.56132 12.77758 20.44060 ## [1297] 12.73428 12.43681 25.07618 26.08877 20.14714 20.69256 ## [1303] 28.25613 12.98901 24.35485 18.42470 16.26899 18.26388 ## [1309] 27.05893 19.86982 19.08220 20.52390 19.32340 13.32630 ## [1315] 18.94521 25.02640 18.75580 20.14197 12.13113 13.86772 ## [1321] 15.86138 18.53696 27.41084 13.16137 24.79052 26.21349 ## [1327] 24.20007 28.54248 24.53781 12.41142 18.98228 22.69294 ## [1333] 27.28046 El error cuadrático medio para el caso óptimo es, library(zoo) ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric library(hydroGOF) rmse(df.pred2, df.test$varobj) ## [1] 14.95591 2.4 KNN aleatorio Primero vamos a calcular el número de clasificadores óptimo. (p=ncol(df.train)-2) ## [1] 40 m=20 #Tomamos 20 variables (rnc=r(p,m,eta=0.9,method=&quot;poisson&quot;)) ## [1] 9 Aplicamos la técnica KNN aleatorio con r = 5 fit.train3 = rknn(data=df.train[,-1], newdata= df.test[,-1],y=df.train$varobj, k = 4, r=5, mtry = m) Veamos las características de este KNN aleatorio, (df.pred3 = as.numeric(paste(fit.train3$pred))) ## [1] 12.64 14.21 14.43 12.13 18.30 25.65 19.81 19.49 12.59 ## [10] 24.41 27.28 11.16 24.17 24.35 13.18 12.13 19.15 13.23 ## [19] 12.03 29.58 17.54 18.14 13.38 23.89 12.35 26.82 19.85 ## [28] 13.52 25.50 12.95 14.75 18.56 23.99 14.67 12.45 12.58 ## [37] 18.75 12.03 30.43 19.65 25.94 12.13 32.85 12.21 12.81 ## [46] 18.67 12.50 28.20 19.41 25.05 19.25 19.01 18.99 14.57 ## [55] 12.58 27.66 19.32 14.78 12.37 26.81 18.66 11.22 17.79 ## [64] 12.13 15.75 19.53 12.13 24.21 12.30 12.03 17.72 27.01 ## [73] 17.78 13.17 27.28 12.20 18.32 16.87 12.30 12.56 19.40 ## [82] 24.52 19.51 12.05 13.00 13.48 18.19 11.91 18.08 18.10 ## [91] 18.55 25.89 24.13 26.64 25.48 14.16 36.35 14.78 26.88 ## [100] 27.32 13.05 21.69 24.41 18.69 12.70 20.89 27.88 14.78 ## [109] 26.89 12.03 19.81 12.61 25.54 11.07 12.92 25.05 26.68 ## [118] 12.03 191.43 19.26 24.52 13.36 19.28 27.77 13.37 24.70 ## [127] 12.99 14.78 13.84 17.44 12.54 31.29 27.74 19.31 26.81 ## [136] 17.77 18.16 12.13 25.24 19.38 26.89 18.66 18.35 24.71 ## [145] 28.19 26.46 19.27 18.87 19.15 26.89 12.17 15.06 12.30 ## [154] 18.44 19.01 13.59 18.95 18.37 12.18 24.50 25.54 12.03 ## [163] 26.68 43.42 12.54 18.66 19.14 14.75 19.32 12.41 19.12 ## [172] 24.78 19.65 28.69 18.19 25.36 12.05 17.61 19.75 18.42 ## [181] 28.20 24.37 19.53 15.75 17.72 26.82 19.44 18.50 12.30 ## [190] 19.89 24.39 25.40 19.90 24.77 19.02 25.24 19.20 20.64 ## [199] 20.64 14.45 12.05 17.95 18.01 12.92 12.30 17.54 18.38 ## [208] 12.88 14.52 18.74 19.35 17.50 11.77 25.89 28.18 15.48 ## [217] 13.45 43.84 13.10 12.30 13.05 19.98 26.24 17.80 12.18 ## [226] 17.80 18.30 28.43 12.90 27.28 18.55 13.46 18.02 11.66 ## [235] 13.42 18.66 19.55 18.88 19.12 12.13 25.25 12.30 18.96 ## [244] 19.15 17.98 12.67 17.50 14.18 17.53 17.54 26.46 18.30 ## [253] 14.67 19.53 12.61 12.30 12.39 24.05 18.54 12.13 12.03 ## [262] 18.54 12.18 19.12 17.54 31.81 18.43 26.89 14.30 25.08 ## [271] 18.92 17.54 18.02 25.01 13.90 14.11 14.78 11.77 18.93 ## [280] 12.03 29.14 18.91 27.74 11.82 18.89 24.86 11.33 24.41 ## [289] 18.50 26.52 13.84 13.18 19.27 12.41 14.52 38.94 11.52 ## [298] 27.64 18.29 27.74 18.66 19.99 11.22 24.52 24.87 18.02 ## [307] 18.91 29.09 18.43 18.30 12.56 12.30 18.52 18.35 18.56 ## [316] 12.37 12.54 23.80 18.00 40.97 11.66 18.61 12.13 12.06 ## [325] 28.24 14.34 18.31 25.11 18.62 26.52 23.46 12.03 24.17 ## [334] 29.52 17.66 18.19 24.41 19.38 12.38 19.68 13.59 10.98 ## [343] 12.30 25.15 18.11 26.59 18.52 13.81 24.47 12.94 12.21 ## [352] 24.52 13.12 23.52 12.37 24.59 26.22 13.46 17.35 15.01 ## [361] 25.63 26.89 13.45 26.81 26.47 18.56 12.30 18.32 25.33 ## [370] 18.16 12.30 24.50 12.03 24.17 23.46 24.41 13.14 12.30 ## [379] 11.69 12.13 17.87 28.19 14.67 17.81 12.32 25.40 14.18 ## [388] 14.78 14.18 12.37 17.44 12.03 18.30 17.35 12.13 30.84 ## [397] 12.61 20.26 13.46 17.44 25.76 12.30 27.01 28.93 17.44 ## [406] 13.36 11.52 14.34 26.81 17.54 19.75 14.92 13.24 26.98 ## [415] 27.61 21.00 18.21 20.26 25.94 24.96 18.55 12.04 13.83 ## [424] 44.68 12.30 23.46 33.86 16.72 12.03 17.54 30.43 19.44 ## [433] 30.82 38.56 18.99 17.72 26.26 18.44 27.53 19.39 14.20 ## [442] 18.39 18.67 18.49 18.14 17.80 38.56 11.88 12.39 25.63 ## [451] 18.58 18.90 23.62 24.59 29.08 20.39 26.59 18.22 18.66 ## [460] 13.05 11.61 12.50 19.75 12.41 19.25 17.80 18.66 14.88 ## [469] 12.68 12.21 24.41 24.41 23.62 18.62 26.89 13.46 18.95 ## [478] 24.77 12.56 18.66 18.55 13.18 18.54 24.59 24.86 12.56 ## [487] 17.80 24.41 11.16 23.23 26.26 27.02 22.60 28.96 24.17 ## [496] 19.65 12.05 17.92 26.82 24.54 14.80 26.96 18.56 18.92 ## [505] 25.01 12.05 24.35 11.40 24.77 19.01 17.51 26.17 23.62 ## [514] 12.05 12.20 20.09 18.55 13.18 12.99 17.66 26.81 27.96 ## [523] 25.94 13.57 24.86 18.51 14.45 12.30 12.91 12.84 19.99 ## [532] 24.09 12.30 24.17 18.32 17.76 12.30 12.08 17.76 18.96 ## [541] 19.89 19.15 20.62 24.92 12.56 19.53 25.33 25.43 17.81 ## [550] 29.76 24.13 24.74 19.23 29.58 24.97 27.17 22.31 18.55 ## [559] 24.41 54.11 17.95 12.30 12.76 31.07 11.88 19.38 24.21 ## [568] 31.81 25.05 24.13 19.32 17.41 17.78 18.66 30.04 24.44 ## [577] 19.04 12.20 18.66 19.96 12.99 38.56 12.18 30.99 27.17 ## [586] 13.00 24.21 17.50 19.32 23.23 23.88 18.80 18.02 18.66 ## [595] 13.61 27.98 17.50 27.64 13.05 13.12 14.51 12.30 18.88 ## [604] 19.32 17.76 12.05 27.91 19.91 17.72 18.83 23.46 19.64 ## [613] 25.54 12.37 22.51 26.45 26.59 26.22 18.91 23.62 48.07 ## [622] 18.40 26.67 19.62 18.54 18.94 26.62 26.58 17.55 12.37 ## [631] 19.49 18.48 19.38 14.52 33.65 24.21 18.50 25.54 17.80 ## [640] 14.45 12.79 14.75 17.55 19.91 26.00 30.72 24.35 11.16 ## [649] 18.29 12.56 14.78 12.30 19.85 11.83 18.21 12.58 24.06 ## [658] 26.81 12.58 17.81 39.29 18.16 12.62 19.38 21.09 11.64 ## [667] 19.58 17.80 13.90 13.00 19.38 17.51 27.90 14.27 24.92 ## [676] 12.58 17.76 12.90 10.98 12.17 24.77 12.08 11.70 17.54 ## [685] 19.20 13.46 23.89 18.76 19.50 19.26 19.77 12.81 19.25 ## [694] 18.15 23.62 13.61 24.41 11.80 13.05 26.52 12.18 26.89 ## [703] 11.22 18.85 13.18 28.20 28.17 11.93 12.81 24.86 14.18 ## [712] 13.45 13.81 19.32 13.37 12.70 17.76 18.01 18.22 11.22 ## [721] 13.48 12.05 12.86 14.85 23.90 18.96 26.43 12.03 17.13 ## [730] 18.66 17.54 25.94 18.79 12.13 24.52 17.54 12.59 18.38 ## [739] 13.01 12.53 27.53 12.08 18.96 11.80 19.53 19.35 12.30 ## [748] 25.33 14.78 26.68 30.83 26.15 19.55 24.09 18.30 18.63 ## [757] 12.18 25.25 14.58 27.01 17.81 11.69 25.54 18.54 13.12 ## [766] 12.05 28.67 12.03 15.01 25.24 13.05 18.45 17.76 24.41 ## [775] 12.61 27.42 18.78 26.89 18.48 11.52 13.00 25.24 19.61 ## [784] 19.32 18.80 14.45 17.54 13.05 13.36 18.20 24.13 18.62 ## [793] 18.00 35.48 19.35 12.75 15.20 18.19 18.50 11.66 18.54 ## [802] 18.94 16.44 27.96 26.22 24.03 12.03 26.52 11.75 12.75 ## [811] 13.46 17.44 26.26 18.02 18.38 14.44 12.60 20.34 12.37 ## [820] 17.66 23.62 18.66 18.35 12.08 18.66 25.24 11.91 17.89 ## [829] 13.48 26.52 29.03 28.52 12.57 25.01 18.50 14.17 18.91 ## [838] 11.22 12.60 12.03 23.23 26.17 26.79 17.54 12.13 13.54 ## [847] 13.10 12.30 19.19 27.96 17.80 18.61 13.86 27.74 17.50 ## [856] 24.95 25.28 29.94 24.41 19.01 26.82 12.92 17.80 18.74 ## [865] 14.78 26.45 25.85 13.46 18.66 23.89 19.15 28.23 17.80 ## [874] 12.17 11.93 17.54 25.54 25.81 26.22 27.91 25.24 25.13 ## [883] 13.89 18.66 11.22 22.51 26.52 29.36 18.86 23.46 24.52 ## [892] 19.67 12.41 13.05 27.02 11.71 19.59 13.26 12.64 14.67 ## [901] 23.46 12.92 19.10 15.86 15.39 12.37 12.13 25.50 17.80 ## [910] 27.74 18.66 20.26 25.89 17.44 12.59 26.81 12.05 14.06 ## [919] 31.65 19.40 24.40 18.34 18.43 19.81 25.76 24.59 12.50 ## [928] 14.45 25.19 26.59 23.88 24.99 12.65 12.05 18.19 12.84 ## [937] 12.37 26.89 25.45 11.88 17.66 23.46 28.20 26.59 18.55 ## [946] 14.58 23.65 26.81 19.32 17.54 22.01 18.11 12.18 18.10 ## [955] 24.42 19.22 20.20 19.31 28.42 13.12 13.24 17.54 14.68 ## [964] 24.59 11.07 20.36 17.66 18.21 21.59 11.82 12.05 13.37 ## [973] 13.12 12.67 12.30 24.89 18.58 24.41 27.74 13.18 16.61 ## [982] 12.56 17.54 13.07 24.13 24.86 25.01 24.88 17.78 17.50 ## [991] 17.54 18.38 22.51 17.72 13.58 18.98 18.54 12.30 11.82 ## [1000] 26.75 13.54 18.83 28.42 18.43 24.76 18.02 25.94 13.97 ## [1009] 17.76 23.87 18.86 19.55 20.05 12.88 24.86 13.39 19.86 ## [1018] 17.53 26.85 12.37 18.62 21.09 17.80 18.66 18.11 43.84 ## [1027] 11.78 15.49 28.98 26.39 26.81 12.75 12.43 18.83 25.43 ## [1036] 11.92 16.99 23.88 26.14 26.09 12.93 18.29 24.75 19.76 ## [1045] 12.35 17.54 12.03 23.78 12.78 24.47 11.58 24.21 12.30 ## [1054] 25.63 16.48 13.46 31.87 13.00 13.26 18.66 12.13 30.43 ## [1063] 14.67 12.18 18.49 18.38 23.46 13.64 18.62 18.80 11.22 ## [1072] 30.57 11.83 28.35 26.45 12.61 24.84 24.83 21.07 26.29 ## [1081] 19.44 12.58 18.77 18.79 20.69 12.93 18.02 26.89 17.54 ## [1090] 23.68 19.15 24.52 26.09 26.89 28.52 25.63 28.93 23.85 ## [1099] 12.85 18.96 26.81 17.80 19.35 26.89 28.35 12.64 24.52 ## [1108] 25.54 12.13 18.17 19.49 26.89 17.76 18.43 12.37 11.58 ## [1117] 17.54 25.40 19.31 12.53 25.01 12.35 18.66 26.81 26.89 ## [1126] 24.41 12.13 23.46 26.28 18.62 24.92 18.11 26.45 18.99 ## [1135] 12.91 12.50 12.18 18.02 13.42 25.94 18.66 18.80 18.08 ## [1144] 13.80 18.08 24.46 12.13 13.12 18.40 24.86 25.76 26.80 ## [1153] 24.52 23.46 24.17 13.30 17.35 12.59 24.99 38.56 24.86 ## [1162] 18.92 26.69 18.02 11.88 12.03 17.79 13.48 26.69 24.89 ## [1171] 25.33 19.49 24.72 24.61 12.60 25.94 11.88 26.43 18.30 ## [1180] 27.01 12.30 18.11 12.30 12.05 26.62 12.12 30.84 18.66 ## [1189] 27.24 18.52 13.44 26.17 19.46 15.11 24.84 18.44 13.48 ## [1198] 13.45 19.21 18.29 24.75 12.30 12.30 18.66 12.60 26.59 ## [1207] 11.77 26.79 11.87 12.13 12.03 31.44 12.62 26.82 25.68 ## [1216] 25.05 14.52 18.91 12.39 18.55 25.36 18.11 19.99 24.92 ## [1225] 19.53 12.90 13.48 25.84 27.28 14.44 19.15 17.90 12.03 ## [1234] 23.76 13.55 26.52 26.17 26.89 18.01 12.64 14.27 17.54 ## [1243] 26.62 24.75 18.89 26.85 27.74 17.80 18.66 26.39 11.52 ## [1252] 25.28 26.17 18.55 17.80 18.89 19.41 19.99 26.59 18.99 ## [1261] 12.30 25.84 27.77 26.52 18.11 26.81 12.18 14.18 25.54 ## [1270] 12.13 18.36 27.28 18.22 12.81 23.89 20.28 26.89 26.59 ## [1279] 18.55 20.62 25.63 18.77 17.98 25.24 25.01 17.54 26.52 ## [1288] 13.95 17.54 12.56 24.44 25.54 12.43 23.50 13.10 18.66 ## [1297] 13.37 12.30 24.79 24.75 19.19 18.94 27.96 12.03 26.22 ## [1306] 19.15 12.88 18.66 26.57 17.72 21.69 17.66 18.00 13.05 ## [1315] 17.92 29.40 18.28 18.38 12.92 12.61 12.30 18.66 32.54 ## [1324] 12.50 24.85 26.17 23.68 28.03 24.13 12.84 18.55 18.52 ## [1333] 24.86 Calculamos el error cuadrático medio para esta predicción, library(zoo) library(hydroGOF) rmse(df.pred3, df.test$varobj) ## [1] 14.64499 El valor del error cuadrático medio usando el método anterior era, rmse(df.pred2, df.test$varobj) ## [1] 14.95591 Concluimos que el modelo basado en un KNN ponderado es mejor que el aleatorio, al menos para la semilla que consideramos al comienzo. Sin embargo el aleatorio se ejecuta mucho mas rápido que el poderado. Podriamos pensar si este tiempo de ejecución adiccional merece la pena comparando los errores resultantes. "]
]
